# hello llm
2025_5_22


## 内容

モデルはどちらかを4bit量子化で使用できる。CPUで実行。
- tinyllama 1B
- arrwmint 4B

前者は小型だし、やや古いので回答はおかしいが、ファイルサイズともに軽いので採用した。tinyllamaで軽く遊んだらarrwmintでも同じことを試すとおもしろい。後者はGemma3ベースのVtuberを想定した日本語追加学習モデルで、かなりまともな回答が返ってくる。

モデルの動作速度であるtoken per secondを比較してみるとよい。一般に、モデルが大きいほど速度は低下するが、実際は、CPUでの行列計算の最適化の程度、メモリの帯域幅、モデルアーキテクチャなどに大きく左右される。

## 実行方法
⚠️Windowsではpythonを3.11にしないとllama-cppのビルドで詰まる。uvで3.11を使ったことがないなら、.python-versionに書かれている数字を変える。

このプロジェクトのルートに行って
```
uv run tinyllama.py
```
or
```
uv run arrwmint.py
```

するとモデルのダウンロードが始まる。

モデルのダウンロードが終わると、コンソール出力が沢山出た後に、LLMとの対話ができるようになるので、「You:」のあとに続いて入力する。やめるときはctrl+Cで止めることができる。
