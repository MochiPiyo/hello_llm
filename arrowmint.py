# main.py â€” ArrowMint 4B (INT4) ã‚’ CPU ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯¾è©±
from pathlib import Path
import os
from huggingface_hub import snapshot_download
from llama_cpp import Llama

# -------- ãƒ¢ãƒ‡ãƒ«æŒ‡å®š --------
REPO  = "mmnga/ArrowMint-Gemma3-4B-YUKI-v0.1-gguf"
FNAME = "ArrowMint-Gemma3-4B-YUKI-v0.1-Q4_K_M.gguf"
MODEL = Path("models") / FNAME

# -------- ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆåˆå›ã®ã¿ï¼‰--------
if not MODEL.exists():                       # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã‘ã‚Œã° DL
    snapshot_download(repo_id=REPO,
                      allow_patterns=FNAME,
                      local_dir="models")

# -------- LLM ã®ãƒ­ãƒ¼ãƒ‰ --------
llm = Llama(
    model_path=str(MODEL),    # GGUF ãƒ‘ã‚¹
    n_ctx=2048,               # æœ€å¤§å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    n_threads=os.cpu_count(), # è«–ç† CPU ã‚³ã‚¢ã‚’å…¨éƒ¨ä½¿ã†
    chat_format="gemma"       # Gemma ç”¨ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æ˜ç¤ºï¼ˆçœç•¥å¯ï¼‰
)

# -------- å¯¾è©±ãƒ«ãƒ¼ãƒ—ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡ºåŠ›ï¼‰ --------
print("ğŸ”® LLM Chat â€” type 'exit' to quit")
history = []  # éå»ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ä¿å­˜ã—ã€æ–‡è„ˆã‚’ç¶­æŒ

while True:
    user = input("You: ")
    if user.lower() in {"exit", "quit"}:
        break  # ãƒ«ãƒ¼ãƒ—çµ‚äº†

    history.append({"role": "user", "content": user})

    assistant_tokens = []
    # stream=True: 1 ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤ chunk ãŒå±Šã
    for chunk in llm.create_chat_completion(
        messages=history,
        max_tokens=256,          # max_tokens: ç”Ÿæˆã™ã‚‹æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆå¿œç­”ã®é•·ã•ã®ä¸Šé™ï¼‰
        temperature=0.7,         # temperature: å‡ºåŠ›ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ï¼ˆä½ã„ã»ã©æ±ºå®šçš„ã€é«˜ã„ã»ã©å¤šæ§˜æ€§ãŒå¢—ãˆã‚‹ï¼‰
        top_p=0.9,               # top_p: nucleus sampling ã®ç¢ºç‡è³ªé‡ä¸Šä½ p ã‚’æ¯é›†åˆã«æ¡ç”¨
        stream=True):             # stream: True ã§é€æ¬¡ãƒãƒ£ãƒ³ã‚¯ã‚’å—ã‘å–ã‚ŠãªãŒã‚‰ç”Ÿæˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒ å‡ºåŠ›ï¼‰
        delta = chunk["choices"][0].get("delta", {})
        if "content" in delta:               # å®Ÿéš›ã®ãƒˆãƒ¼ã‚¯ãƒ³
            token = delta["content"]
            print(token, end="", flush=True) # å³æ™‚è¡¨ç¤º
            assistant_tokens.append(token)
    print()                                   # å¿œç­”çµ‚äº† â†’ æ”¹è¡Œ

    answer = "".join(assistant_tokens)        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é€£çµã—ã¦å…¨æ–‡ã«
    history.append({"role": "assistant", "content": answer})